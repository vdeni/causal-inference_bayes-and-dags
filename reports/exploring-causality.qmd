---
title: 'Exploring causal inference with DAGs and Bayesian models'
author: Denis Vlašiček
date: today
execute:
    cache: true
bibliography: references.bib
highlight-style: gruvbox-dark
format:
    html:
        toc: true
        toc-location: left
        theme: litera
        code-overflow: wrap
---

Lately, I've been reading quite a lot about causal inference in statistics and
the various approaches developed through the decades. Pearl's framework, which
relies on directed acyclic graphs (DAGs), resonated with me the most; I really
like DAGs as a way of expressing ideas about causality, and as a tool for
tackling causal questions and building causal models.

As I've been focused on Bayesian modeling for the past couple of years, I've
found the generative modeling approach to be quite a useful way of approaching
data analysis. The directed acyclic graph approach and the generative modeling
approach seem to have a lot in common. However, I still feel like a difference
is being made between the two. Because of this (and other pedagogic reasons),
I've decided to explore some simple causal setups to gain (hopefully) a better
understanding of both causal inference and the DAG/generative modeling
frameworks.

# Case 1 - three variables with simple confounding

The first case I'm going to look at is a simple three-variable case where one
variable acts as a confounder of the causal relationships of the remaining two
variables. We'll have a variable $X$ that exerts a causal influence on a
variable $Y$. We'll also have a variable $Z$ that has a causal effect on both
$X$ and $Y$, making $Z$ a confounder of the causal relationship between
$X$ and $Y$. This model can be represented by the following directed acyclic
graph (DAG), where each arrow represents the causal effect which the variable
at the arrow's tail has on the variable at the arrow's head (e.g. $X$ has a
causal effect on $Y$):

```{dot fig_dag_no-coef}
//| fig-align: center
//| fig-cap: Three variables, simple confounding.
//| label: fig-three-var-confounding

digraph G {
    layout=dot;
    rankdir=TB;
    X -> Y[minlen=4];
    Z -> X;
    Z -> Y;
    {rank="same"; X; Y;}
}
```

In the graph pictured in Figure @fig-three-var-confounding
there are two paths between $X$ and $Y.$ The first path is the direct one, going
from $X$ to $Y$ ($X \textrightarrow Y$). The second path is an indirect one,
and goes over the $Z$ variable: $X \textleftarrow Z \textrightarrow Y$.

I've mentioned earlier that the direction of the arrows tells us the direction
of the causal effects. Therefore, we can say that the path
$X \textrightarrow Y$ is causal. However, the path
$X \textleftarrow Z \textrightarrow Y$ is not a causal path --- starting from
the left, we have an arrow going into $X$ from $Z$, and then an arrow going from
$Z$ into $Y$. Since this path starts with an arrow pointing *into* $X$, this
makes it a *backdoor path* between $X$ and $Y$.

The idea of backdoor paths is one of the central ideas in Pearl's framework.
Backdoor paths are not causal, but create problems when trying to estimate
causal effects. In the graph in Figure @fig-three-var-confounding, we'd have to
somehow close the backdoor path in order to obtain an unbiased estimate of the
causal effect of $X$ on $Y$. In this case, the path can be close by adjusting
for the $Z$ variable. We'll see how to do that in a bit.

I'll first set up the Julia environment and simulate some values for the
variables $X$, $Z$ and $Y$.

```{julia setup}
#| output: false

using Pkg

Pkg.activate("..")

using Distributions
using Random
using Statistics
using StatsModels
using Turing
using GLM
using DataFrames
using Markdown
using StatsPlots
using QuadGK

include(joinpath("..",
                 "helpers",
                 "text-formatting.jl"))

include(joinpath("..",
                 "helpers",
                 "plots.jl"))

Random.seed!(1)
```

Now, let's simulate data from the data generating process described in
@fig-three-var-confounding. For simplicity, I'll assume all variables are
distributed normally with the same standard deviation $\sigma = 1$, but with
different means. Also for simplicity, I'll assume that the relationships between
the variables can be described by a simple linear model. Therefore, we have:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &\sim \text{normal}(\mu_{x_i}, 1) \\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{x_i} &= 3 z_i \\
    \mu_{y_i} &= 2 x_i + 4 z_i
\end{align}

```{dot fig_dag_effects}
//| fig-cap: Causal effects for three variables, simple confounding scenario.
//| label: fig-three-var-confounding-effs

digraph effects_G {
    layout=dot;
    bgcolor="transparent";
    rankdir=TB;
    X -> Y [label="b_yx = 2", minlen=4];
    Z -> X [label="b_xz = 3"];
    Z -> Y [label="b_yz = 4"];
    {rank="same"; X; Y;}
}
```

With that in mind, I'll simulate the data and put it in a data frame:

```{julia generate-data}
Z_distr = Distributions.Normal(1, 1);

N = 1000 # number of simulated draws

Z = Random.rand(Z_distr,
                N)

# create X values
x_mu = 3 * Z

X_distr = Distributions.Normal.(x_mu, 1)

X = Random.rand.(X_distr)

# create Y values
y_mu = 2 * X + 4 * Z

Y_distr = Distributions.Normal.(y_mu, 1)

Y = Random.rand.(Y_distr)

d = DataFrames.DataFrame("Y" => Y,
                         "X" => X,
                         "Z" => Z)

d[1:10, :]
```

Pearl's DAG framework tells us that fitting a, in this case, linear model
with $X$ as a predictor and $Y$ as criterion would give us a biased estimate of
the causal effect of $X$ on $Y$ due to the backdoor path over $Z$
($X \textleftarrow Z \textrightarrow Y$). Therefore, fitting a linear regression
model should give an estimate of the $b$ coefficient for $X$ different than $2$,
which was coded while creating the data.

```{julia fit_confounded}
f = StatsModels.@formula(Y ~ X)

fit_y_x = GLM.lm(f,
                 d)
```

```{julia}
#| echo: false

x_idx = findall(x -> x == "X",
                coefnames(fit_y_x))[1]

Markdown.parse("""
               As can be seen, we get
               \$X = $(coef(fit_y_x)[x_idx] |> round_2)\$, which is different
               than the value set in the simulation.
               """)
```

According to Pearl's framework, we should also be able to get to the causal
effect of $X$ on $Y$ if we control for the confounder $Z$. Therefore, let's try
fitting another model, this time including $Z$ as a variable in the regression
model alongside $X$.

```{julia fit_non-confounded}
f = StatsModels.@formula(Y ~ X + Z)

fit_y_xz = GLM.lm(f,
                  d)
```

Now we get a $b_{YX}$ coefficient which corresponds to the one we've coded up
while simulating the data. The estimate of the effect of $Z$ on $Y$ is also in
line with the value which we've coded up.

Okay, now to the part that's a bit more tricky. I want to explore the
relationships between Bayesian networks and causal directed acyclic graphs.
If my understanding is correct, a Bayesian network is, in a way, a subset of a
causal directed acyclic graph --- it encodes the relationships between the
variables under investigation, but doesn't make additional assumptions which are
necessary for causal inference. However, a Bayesian network that corresponds to
the true DAG should be able to provide us with unbiased estimates of the causal
effects. A Bayesian network that does not correspond to the true DAG, on the
other hand, should provide biased estimates of the causal effects, but should
still be able to make accurate predictions. This may be a fine distinction, and
is something I also aim to explore further down.

I'll fit the model represented in Figure @fig-three-var-confounding-effs
using the `Turing.jl` library. The model can be coded as follows:

```{julia model_three-var-simple-confound}
Turing.@model function gen_model_y_xz(X,
                                      Y,
                                      Z)
    s ~ Turing.Exponential(1)

    b_XZ ~ Turing.Normal(2, 1)
    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    Z ~ Turing.Normal(2, s)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_XZ,
                             s)

        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

Next, we tell `Turing` to sample using the NUTS sampler, which is the same one
*Stan* uses:

```{julia sample_three-var-simple-confound}
#| message: false

chains = Turing.sample(gen_model_y_xz(d.X,
                                      d.Y,
                                      d.Z),
                       Turing.NUTS(1000,
                                   0.80),
                       Turing.MCMCThreads(),
                       3000,
                       4)
```

Indeed, the parameters estimated in this way correspond to the ones we've
simulated earlier. Now, this probably isn't that surprising, given that our
model is coded in line with the true DAG. Still, that *is* comforting, I guess.

I'll try repeating the analysis using a distribution for the $Z$ variable which
I know to be incorrect - the Cauchy distribution with location 3 and scale 1.

```{julia model_three-var-simple-confound_cauchy}
Turing.@model function gen_model_y_xz_cauchy(X,
                                             Y,
                                             Z)
    s ~ Turing.Exponential(1)

    b_XZ ~ Turing.Normal(2, 1)
    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    Z ~ Turing.Cauchy(3, 1)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_XZ,
                             s)

        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

```{julia sample_three-var-simple-confound_cauchy}
chains_cauchy = Turing.sample(gen_model_y_xz_cauchy(d.X,
                                                    d.Y,
                                                    d.Z),
                              Turing.NUTS(1000,
                                          0.80),
                              Turing.MCMCThreads(),
                              3000,
                              4)
```

Luckily, this also seems to work. So we can be somewhat off.

Now, let's examine a DAG we know to be false (a simple Bayesian network, I
guess), and compare the predictions of a model that's coded in line with that
DAG, with those of the model coded according to the true DAG. If my
understanding is correct, both the causal DAG and the Bayesian network should be
able to make predictions of similar accuracy. That would mean that values
$\hat{y}^{cDAG}$ predicted from the causal DAG should be very similar (or
identical) to the values $\hat{y}^{BN}$ predicted by a Bayesian network.

To check this claim, I'll try modeling the data according to the following
graph, which we know to be false:

```{dot fig_bn_x-confound}
//| fig-align: center
//| fig-cap: The obviously false DAG.
//| label: fig-three-var-confounding-x-confound

digraph G {
    layout=dot;
    Z[rank="min"];
    rankdir=TB;
    X -> Y[minlen=4];
    X -> Z[color="red"];
    Z -> Y;
    {rank="same"; X; Y;}
}
```

```{julia model_bn_x-confound}
Turing.@model function bn_model_y_xz(X,
                                     Y,
                                     Z)
    s ~ Turing.Exponential(1)

    b_ZX ~ Turing.Normal(2, 1)
    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    X ~ Turing.Normal(2, 1)

    for i in eachindex(X)
        Z[i] ~ Turing.Normal(X[i] * b_ZX,
                             s)

        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

```{julia sample_bn_x-confound}

chains_x_conf = Turing.sample(bn_model_y_xz(d.X,
                                            d.Y,
                                            d.Z),
                              Turing.NUTS(1000,
                                          0.80),
                              Turing.MCMCThreads(),
                              3000,
                              4)
```

So, the coefficients for the effect of $X$ on $Y$ and $Z$ on $Y$ are actually
quite fine. The coefficient for the effect of $X$ on $Z$ is, of course,
gibberish, if interpreted causally. But how do the predicted $Y$ values from
this model compare to the values predicted from the true DAG?

Let's compare each of these models' $\hat{y}$ values to the observed $Y$ values,
as well as the models' predictions to each other. I'll use only the mean
estimates of the parameters, disregarding the uncertainty.

```{julia fig_compare_dag-bn-x-confound}
pred_gen_model_y_xz = gen_model_y_xz(d.X,
                                     fill(missing,
                                          N),
                                     d.Z)

y_hat_dag = Turing.predict(pred_gen_model_y_xz,
                           chains)

pred_bn_model_y_xz = bn_model_y_xz(d.X,
                                   fill(missing,
                                        N),
                                   d.Z)

y_hat_bn = Turing.predict(pred_bn_model_y_xz,
                          chains_x_conf)

d_dag_bn = DataFrames.DataFrame("Y_dag" =>
                                    MCMCChains.summarystats(y_hat_dag)[:, :mean],
                                "Y_bn" =>
                                    MCMCChains.summarystats(y_hat_bn)[:, :mean],
                                "Y_obs" => Y)

p_dag_obs = compare_yhat(d_dag_bn;
                         y_pred_1_name = "Y_dag",
                         y_pred_2_name = "Y_obs")

p_bn_obs = compare_yhat(d_dag_bn;
                        y_pred_1_name = "Y_bn",
                        y_pred_2_name = "Y_obs")

p_dag_bn = compare_yhat(d_dag_bn)

plot(p_dag_obs,
     p_bn_obs,
     p_dag_bn,
     layout = (3, 1))
```

Aside from seemingly being very good at predicting the $Y$ values, both models
seem to make, for all practical intents and purposes (or just "ignoring random
differences due to the stochastic nature of MCMC"), identical predictions.

But what does this all mean for predicting the effects of interventions, as one
would do using Pearl's *do*-calculus, which assume knowledge of the causal
 mechanism? According to Pearl
[@pearlCausalInferenceStatistics2016] an intervention can be represented by a
DAG from which all the arrows going into the intervened upon variable are
removed. For example, here, we'll try predicting the effect of an intervention
that would set the value of $X$ to some $x$, denoted $do(X = x)$.
Under such an intervention, our two competing DAGs would look like this:

```{dot fig_bn_x-confound}
//| fig-align: center
//| fig-cap: Causal DAGs when we simulate a $do(X = x)$ intervention. True DAG left, false DAG right.
//| label: fig-simple-confounding-do-compare

digraph G {
    layout=dot;
    subgraph dag_true {
        rankdir=TB;
        X1[label="X"];
        Y1[label="Y"];
        Z1[label="Z"];
        X1 -> Y1[minlen=4];
        Z1 -> X1[style="invisible", arrowhead="none"];
        Z1 -> Y1;
        {rank="same"; X1; Y1;}
    }
    subgraph dag_false {
        rankdir=TB;
        X2[label="X"];
        Y2[label="Y"];
        Z2[label="Z"];
        X2 -> Y2[minlen=4];
        Z2 -> Y2;
        X2 -> Z2;
        {rank="same"; X2; Y2;}
    }
    {rank="same"; Z1; Z2;}
}
```

So, we could ask, for example, what is the expected value of the $Y$ variable if
we were to conduct an intervention that would set the value of $X$ to 6. We are
interested in $\text{E}(Y | do(X = 6))$.

Given that we know the equations of the true data generating process, we can
calculate this. We know that each $y$ value is a draw from a normal distribution
with location $\mu_{y_i}$ and scale $1$. We further know that the value of
$\mu_{y_i}$ is determined by the values which the variables $X$ and $Z$ take for
a chosen $i$ (i.e. the values $X = x_i$ and $Z = z_i$).

Since we've made the intervention $do(X = 6)$, we can just replace $x_i$ with
$6$. However, we cannot just disregard the $Z$ variable, given that it is not
independent of $Y$. Therefore, we will average over the $Z$ values.

This was our starting model specification:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &\sim \text{normal}(\mu_{x_i}, 1) \\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{x_i} &= 3 z_i \\
    \mu_{y_i} &= 2 x_i + 4 z_i
\end{align}

We'll replace the second line with a constant:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &= 6\; \text{for all}\; i\\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{y_i} &= 2 x_i + 4 z_i \\
\end{align}

We've said earlier that the expected value of $Y$ given some values of $X$ and
$Z$ is equal to $\mu_{y_i}$:

\begin{equation}
    \mu_{y_i} = \text{E}(Y | X, Z).
\end{equation}

Finally, given that we've set $X = 6$ through an intervention (and thus replaced
$x_i \sim \text{normal}(\mu_{x_i}, 1)$ with a constant), we have:

\begin{equation}
    \text{E}(Y | X = 6, Z) = 2 \times 6 + 4 z_i
\end{equation}

Finally, we want to average over the values of $Z$ to get $\text{E}(Y | X)$:

\begin{align}
    \text{E}(Y | X) &= \text{E}(\text{E}(Y | X, Z)) \\
                    &= \int_{z \in Z}  \text{E}(Y | X = 6, Z = z) f_Z(z)\;
                        \text{d}z \\
                    &= \int_{z \in Z}(2 \times 6 + 4z)f_Z(z)\, \text{d}z.
\end{align}

We can use the `QuadGK.jl` package to calculate this expectation:

```{julia}
function integral(z)
    f_Z = Distributions.Normal(1, 1)
    return (2 * 6 + 4 * z) * Distributions.pdf(f_Z, z)
end

QuadGK.quadgk(integral,
              -Inf,
              Inf)
```

To confirm that our calculation is correct, we'll also simulate a new set of $Y$
values, with $X = 6$ (we'll rely on some of the variables created in the first
simulation, and we'll increase the number of simulated draws):

```{julia}
N_sim = 100000

x_i = 6

Z_prime = Random.rand(Z_distr,
                      N_sim)

# create Y values
y_mu = 2 * x_i .+ 4 * Z_prime

Y_x = Distributions.Normal.(y_mu, 1)

Y_x_fixed = Random.rand.(Y_x)

mean(Y_x_fixed)
```

As we can see, we're getting a very similar result. This gives us additional
confidence that our calculation was correct.

Now, what's the deal with the two models --- how do we obtain $\text{E}(Y | do(X
= 6), Z)$? Let's first look at the model corresponding to the true
data-generating process. According to @pearlCausalInferenceStatistics2016, to
estimate the causal effect an intervention on $X$ ($do(X = 6)$) would have on
$Y$, we have to adjust for $X$'s parent variable $Z$.
We do that the same way as when we were calculating the true expected value:

\begin{align}
    \text{E}(Y | do(X = 6), Z) &= \int_{z \in Z}\text{E}(Y | X = 6, z) f_Z(z)\,
        \text{d}z \\
    \text{E}(Y | do(X = 6), Z) &= \int_{z \in Z}(\beta_{YX} \times 6 +
        \beta_{YZ} \times z)f_Z(z)\, \text{d}z.
\end{align}

```{julia}
#| echo: false
Markdown.parse("""Let's take the posterior means of the parameter estimates
               for \$\\beta_{YX}\$ and \$\\beta_{YZ}\$, that is
               \$\\beta_{YX} =
               $(MCMCChains.summarystats(chains)[3, :mean] |> round_2)\$
               and \$\\beta_{YZ} =
               $(MCMCChains.summarystats(chains)[4, :mean] |> round_2)\$""")
```

```{julia}
b_yx = MCMCChains.summarystats(chains)[3, :mean]
b_yz = MCMCChains.summarystats(chains)[4, :mean]

function integral(z)
    f_Z = Distributions.Normal(1, 1)
    return (b_yx * 6 + b_yz * z) * Distributions.pdf(f_Z, z)
end

QuadGK.quadgk(integral,
              -Inf,
              Inf)
```

Alternatively, instead of relying the fact the we know the distribution of $Z$,
we could do:

```{julia}
1 / N * sum(b_yx * 6 .+ b_yz * Z)
```

The results align perfectly with the one obtained from the true model, which
could have been expected, given that we've estimated both $\beta_{YX}$ and
$\beta_{YZ}$ correctly. So we basically just reran the previous calculation.

Now for the second model. As we've mentioned earlier, no arrows get deleted
in this model, as there are no arrows entering $X$. This also means that we
won't be adjusting for $Z$ in this case (i.e. we won't be integrating over the
$z$ values). To obtain $\text{E}(Y | do(X = 6), Z)$, we have

\begin{align}
    \text{E}(Y | do(X = 6), Z) &= \text{E}(Y | X = 6, Z) \\
        &= \beta_{YX} \times 6 + \beta_{YZ} \times Z.
\end{align}

However, we have to be mindful of one thing: according to our model, $Z$ is
under the direct influence of $X$. Therefore, if $X$ is set to some value $x$
through an intervention, $Z$ also changes. We have to keep that in mind if we
want to get an estimate of the total causal effect $X$ has on $Y$ --- both the
direct effect, and the indirect effect going through $Z$. So we'll modify the
previous equation, and mark the new variable as $Z'$, keeping in mind that it
represents the values that the $Z$ variable would obtain if the $X$ values
were set to some $x$ (in our case, 6)[^counterfactuals].

\begin{align}
    \text{E}(Y | do(X = 6), Z') &= \text{E}(Y | X = 6, Z') \\
        &= \beta_{YX} \times 6 + \beta_{YZ} \times Z'.
\end{align}

We also have to keep in mind that $Z'$ is a variable. So, to finally obtain the
estimate of $\text{E}(Y | do(X = 6), Z)$ we can simulate a set of $Z'$ values
and calculate

$$
\frac{1}{N} \sum_{i}^N (\beta_{YX} \times 6 + \beta_{YZ} \times z'_i)
$$

```{julia}
b_zx = MCMCChains.summarystats(chains_x_conf)[2, :mean]
b_yx = MCMCChains.summarystats(chains_x_conf)[3, :mean]
b_yz = MCMCChains.summarystats(chains_x_conf)[4, :mean]

Z_prime_distr = Distributions.Normal(b_zx * 6, 1)

Z_prime = Random.rand(Z_prime_distr,
                      10_000)

mean(b_yx * 6 .+ b_yz * Z_prime)
```

Alternatively, if we rearrange the previous equation, we'll notice that we're
really working with the expected value of the $Z'$ variable:

\begin{align}
    \text{E}(Y | X = 6, Z') &= \frac{1}{N} \sum_{i}^N (\beta_{YX} \times 6 +
        \beta_{YZ} \times z'_i) \\
    &= \beta_{YX} \times 6 + \beta_{YZ} \times
        \underbrace{\frac{1}{N} \sum_i^N z'_i}_{\text{E(Z')}}.
\end{align}

According to our model, that is:

$$
    \text{E}(Z') = \mu_{z'_i} = \beta_ZX \times x_i,\; \text{where}\;
        x_i = 6\; \forall\, i
$$

meaning that we can also estimate the total causal effect of $do(X = x)$ on
$Y$ like this:

```{julia}
#                   E(Z')
b_yx * 6 + b_yz * (b_zx * 6)
```

Here, we get an estimate of the total causal effect of $X$ on $Y$ that's clearly
biased upwards.

```{julia fig_compare_dag-bn-x-confound_intervention}
pred_gen_model_y_xz = gen_model_y_xz(fill(6,
                                          N),
                                     fill(missing,
                                          N),
                                     d.Z)

y_hat_dag = Turing.predict(pred_gen_model_y_xz,
                           chains)

pred_bn_model_y_xz = bn_model_y_xz(fill(6,
                                        N),
                                   fill(missing,
                                        N),
                                   fill(missing,
                                        N))

y_hat_bn = Turing.predict(pred_bn_model_y_xz,
                          chains_x_conf)


y_hat_bn_summary = MCMCChains.summarystats(y_hat_bn) |>
    DataFrames.DataFrame

y_hat_bn_summary = transform(y_hat_bn_summary,
                             :parameters => x -> String.(x);
                             copycols=false,
                             renamecols=false)

y_hat_bn_summary = subset(y_hat_bn_summary,
                          :parameters => x -> contains.(x, r"^Y"))

d_dag_bn = DataFrames.DataFrame("Y_dag" =>
                                    MCMCChains.summarystats(y_hat_dag)[:, :mean],
                                "Y_bn" => y_hat_bn_summary[:, :mean])

compare_yhat(d_dag_bn) |> display

DataFrames.combine(d_dag_bn,
                   DataFrames.All() .=> mean)
```

Here's something interesting. While the model corresponding to the true DAG
makes variable predictions (whose mean matches the expectation we've calculated
earlier), the wrong model predicts values to be in a very narrow range (and
which also have an overestimated expected value, also as calculated earlier).
[^disclaimer_compare]

[^counterfactuals]: Hello, counterfactuals!
[^disclaimer_compare]: I do have to put a disclaimer --- since I'm really new
to both Julia and `Turing.jl`, it could easily be the case that I've coded
something model incorrectly.
