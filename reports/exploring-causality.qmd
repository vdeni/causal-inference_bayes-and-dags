---
title: 'Exploring causal inference with DAGs and Bayesian models'
author: Denis Vlašiček
date: today
execute:
    cache: true
---

Lately, I've been reading quite a lot about causal inference in statistics and
the various approaches developed through the decades. Pearl's framework, which
relies on directed acyclic graphs (DAGs) resonated with me the most; I really
like DAGs as a way of expressing ideas about causality, and as a tool for
tackling causal questions and building causal models.

As I've been focused on Bayesian modeling for the past couple of years, I've
found the generative modeling approach to be quite a useful way of approaching
data analysis. The directed acyclic graph approach and the generative modeling
approach seem to have a lot in common. However, I still feel like a difference
is being made between the two. Because of this (and other pedagogic reasons),
I've decided to explore some simple causal setups to gain (hopefully) a better
understanding of both causal inference and the DAG/generative modeling
frameworks.

# Case 1 - three variables with simple confounding

The first case I'm going to look at is a simple three-variable case where one
variable acts as a confounder of the causal relationships of the remaining two
variables. I'm talking about the case pictured below:

```{dot fig_dag_no-coef}
//| fig-align: center
//| fig-cap: Three variables, simple confounding.
//| label: fig-three-var-confounding

digraph G {
    layout=dot;
    rankdir=LR;
    X -> Y;
    Z -> X;
    Z -> Y;
}
```

So, I'm assuming that the $X$ variable exerts a causal effect on the $Y$
variable, and that the $Z$ variable has a causal effect both on $X$ and $Y$.
Therefore, $Z$ creates a backdoor path from $X$ to $Y$, which means that it has
to be adjusted for in order to get an unbiased estimate of the causal effect of
$X$ on $Y$.

Before simulating some values for the variables, I'll load some Julia packages:

```{julia setup}
using Pkg

Pkg.activate("..")

using Distributions
using Random
using Statistics
using StatsModels
using Turing
using GLM
using DataFrames
using Markdown
using StatsPlots

include(joinpath("..",
                 "helpers",
                 "text-formatting.jl"))

include(joinpath("..",
                 "helpers",
                 "plots.jl"))

Random.seed!(1)
```

Now, let's simulate data from the data generating process described in
@fig-three-var-confounding. For simplicity, I'll assume all variables are
distributed normally with the same standard deviation $\sigma = 1$, but with
different means. Also for simplicity, I'll assume that the relationships between
the variables can be described by a simple linear model. Therefore, we have:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &\sim \text{normal}(\mu_{x_i}, 1) \\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{x_i} &= 3 z_i \\
    \mu_{y_i} &= 2 x_i + 4 z_i
\end{align}

```{dot fig_dag_effects}
//| fig-cap: Causal effects for three variables, simple confounding scenario.
//| label: fig-three-var-confounding-effs

digraph effects_G {
    layout=dot;
    rankdir=LR;
    X -> Y [label="b_yx = 2"];
    Z -> X [label="b_xz = 3"];
    Z -> Y [label="b_yz = 4"];
}
```

With that in mind, I'll simulate the data and put it in a data frame:

```{julia generate-data}
Z_distr = Distributions.Normal(3, 1);

N = 1000 # number of simulated draws

Z = Random.rand(Z_distr,
                N)

# create X values
x_mu = 3 * Z

X_distr = Distributions.Normal.(x_mu, 1)

X = Random.rand.(X_distr)

# create Y values
y_mu = 2 * X + 4 * Z

Y_distr = Distributions.Normal.(y_mu, 1)

Y = Random.rand.(Y_distr)

d = DataFrames.DataFrame("Y" => Y,
                         "X" => X,
                         "Z" => Z)

d[1:10, :]
```

Now, Pearl's DAG framework tells us that fitting a, in this case, linear model
with $X$ as a predictor and $Y$ as criterion would give us a biased estimate of
the causal effect of $X$ on $Y$ due to the backdoor path over $Z$
($X \textleftarrow Z \textrightarrow Y$). Therefore, fitting a linear regression
model should give an estimate of the $b$ coefficient for $X$ different than $2$,
which was coded while creating the data.

```{julia fit_confounded}
f = StatsModels.@formula(Y ~ X)

fit_y_x = GLM.lm(f,
                 d)
```

```{julia}
#| echo: false

x_idx = findall(x -> x == "X",
                coefnames(fit_y_x))[1]

Markdown.parse("""
               As can be seen, we get
               \$X = $(coef(fit_y_x)[x_idx] |> round_2)\$, which is different
               than the value set in the simulation.
               """)
```

According to Pearl's framework, we should also be able to get to the causal
effect of $X$ on $Y$ if we control for the confounder $Z$. Therefore, let's try
fitting another model, this time including $Z$ as a variable in the regression
model alongside $X$.

```{julia fit_non-confounded}
f = StatsModels.@formula(Y ~ X + Z)

fit_y_xz = GLM.lm(f,
                  d)
```

Now we get a $b_{YX}$ coefficient which corresponds to the one we've coded up
while simulating the data. The estimate of the effect of $Z$ on $Y$ is also in
line with the value which we've coded up.

Okay, now to the part that's a bit more tricky. I want to explore the
relationships between Bayesian networks and Bayesian generative models. If my
understanding is correct, a Bayesian generative model specified in line with the
actual data generating mechanism should be able to provide unbiased estimates of
causal effects. I'll try coding up the model represented by the DAG using
`Turing.jl`.

```{julia model_three-var-simple-confound}
Turing.@model function gen_model_y_xz(X,
                                      Y,
                                      Z)
    s ~ Turing.Exponential(1)

    b_XZ ~ Turing.Normal(2, 1)
    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    Z ~ Turing.Normal(2, s)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_XZ,
                             s)
 
        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

And sample from the model:

```{julia sample_three-var-simple-confound}
#| message: false
#| cache: true

chains = Turing.sample(gen_model_y_xz(d.X,
                                      d.Y,
                                      d.Z),
                       Turing.NUTS(1000,
                                   0.80),
                       Turing.MCMCThreads(),
                       3000,
                       4)
```

Indeed, the parameters estimated in this way correspond to the ones we've
simulated earlier. Now, this probably isn't that surprising, given that what
we're doing is, basically, generating the data anew. Still, that *is*
comforting, I guess.

I'll try repeating the analysis using a distribution for the $Z$ variable which
I know to be incorrect - the Cauchy distribution with location 3 and scale 1.

```{julia model_three-var-simple-confound_cauchy}
Turing.@model function gen_model_y_xz_cauchy(X,
                                             Y,
                                             Z)
    s ~ Turing.Exponential(1)

    b_XZ ~ Turing.Normal(2, 1)
    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    Z ~ Turing.Cauchy(3, 1)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_XZ,
                             s)
 
        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

```{julia sample_three-var-simple-confound_cauchy}
#| cache: true

chains_cauchy = Turing.sample(gen_model_y_xz_cauchy(d.X,
                                                    d.Y,
                                                    d.Z),
                              Turing.NUTS(1000,
                                          0.80),
                              Turing.MCMCThreads(),
                              3000,
                              4)
```

Luckily, this also seems to work. So we can be somewhat off.

What I want to wrap my head around next is the relationship between causal DAGs
and Bayesian networks. If my understanding is correct, they should be pretty
similar, with the main difference being that causal DAGs make certain
assumptions regarding the data generating process, which allow us to make causal
claims. However, if my understanding is correct, both causal DAGs and Bayesian
networks should be able to make predictions of similar accuracy. That would mean
that values $\hat{y}^{c-DAG}$ predicted from the causal DAG should be very
similar to the values $\hat{y}^{BN}$ predicted by a Bayesian network.

To check this claim, I'll try modeling the data according to the following
graph, which we know to be false:

```{dot fig_bn_x-confound}
//| fig-align: center
//| fig-cap: The obviously false DAG.
//| label: fig_three-var-confounding-x-confound

digraph G {
    layout=dot;
    rankdir=LR;
    X -> Y;
    X -> Z;
    Z -> Y;
}
```

```{julia model_bn_x-confound}
Turing.@model function bn_model_y_xz(X,
                                     Y,
                                     Z)
    s ~ Turing.Exponential(1)

    b_ZX ~ Turing.Normal(2, 1)
    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    X ~ Turing.Normal(2, 1)

    for i in eachindex(X)
        Z[i] ~ Turing.Normal(X[i] * b_ZX,
                             s)
 
        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

```{julia sample_bn_x-confound}
#| cache: true

chains_x_conf = Turing.sample(bn_model_y_xz(d.X,
                                            d.Y,
                                            d.Z),
                              Turing.NUTS(1000,
                                          0.80),
                              Turing.MCMCThreads(),
                              3000,
                              4)
```

So, the coefficients for the effect of $X$ on $Y$ and $Z$ on $Y$ are actually
quite fine. The coefficient for the effect of $X$ on $Z$ is, of course,
gibberish. But how do the predicted $Y$ values from this model compare to the
values predicted from the true DAG?

```{julia fig_compare_dag-bn-x-confound}
pred_gen_model_y_xz = gen_model_y_xz(d.X,
                                     fill(missing,
                                          N),
                                     d.Z)

y_hat_dag = Turing.predict(pred_gen_model_y_xz,
                           chains)

pred_bn_model_y_xz = bn_model_y_xz(d.X,
                                   fill(missing,
                                        N),
                                   d.Z)

y_hat_bn = Turing.predict(pred_bn_model_y_xz,
                          chains_x_conf)

d_dag_bn = DataFrames.DataFrame("Y_dag" =>
                                    MCMCChains.summarystats(y_hat_dag)[:, :mean],
                                "Y_bn" =>
                                    MCMCChains.summarystats(y_hat_bn)[:, :mean])

compare_yhat(d_dag_bn)
```

For all practical intents and purposes (or just "ignoring random differences due
to the stochastic nature of MCMC"), the two models yield identical predictions.

I'll try fitting another Bayesian network, this one a bit more degenerate. I'll
assume that the $X$ and $Z$ variables are independent, but that they both affect
the $Y$ variable:

```{dot fig_bn_z-x-indep}
//| fig-align: center
//| fig-cap: Even worse DAG.
//| label: fig-three-var-confounding

digraph G {
    layout=dot;
    rankdir=LR;
    X -> Y;
    Z -> Y;
}
```

```{julia model_bn_z-x-indep}
Turing.@model function bn_model_y_xz_indep(X,
                                           Y,
                                           Z)
    s ~ Turing.Exponential(1)

    b_YX ~ Turing.Normal(2, 1)
    b_YZ ~ Turing.Normal(2, 1)

    X ~ Turing.Normal(2, 1)
    Z ~ Turing.Normal(2, 1)

    for i in eachindex(X)
        Y[i] ~ Turing.Normal(X[i] * b_YX + Z[i] * b_YZ,
                             s)
    end
end
```

```{julia sample_bn_x-z-indep}
#| cache: true

chains_xz_indep = Turing.sample(bn_model_y_xz_indep(d.X,
                                                    d.Y,
                                                    d.Z),
                                Turing.NUTS(1000,
                                            0.80),
                                Turing.MCMCThreads(),
                                3000,
                                4)
```

Interestingly, the estimated coefficients seem to correspond to the ones I've
coded up. Again, let's compare the predictions:

```{julia fig_compare_dag-bn-indep}
pred_bn_model_y_xz_indep = bn_model_y_xz_indep(d.X,
                                               fill(missing,
                                                    N),
                                               d.Z)

y_hat_bn = Turing.predict(pred_bn_model_y_xz_indep,
                          chains_xz_indep)

d_dag_bn = DataFrames.DataFrame("Y_dag" =>
                                    MCMCChains.summarystats(y_hat_dag)[:, :mean],
                                "Y_bn" =>
                                    MCMCChains.summarystats(y_hat_bn)[:, :mean])

compare_yhat(d_dag_bn)
```

Much to my surprise, these two also make identical predictions! So, it would
seem that Bayesian networks and generative models fit to causal DAGs *do* lead
to the same predicted values. This may be especially important since the
generative model was fit in accordance with the true data-generating process,
while the Bayesian network was not.

However, what further interests me is whether there's a difference between the
two models' predictions when we simulate an intervention. That is, if we were to
*set* $X$ equal to some value $x$, would our models predict the same $Y$ value?

According to my understanding of causal DAGs, such an intervention would
correspond to creating a mutilated graph, with all arrows pointing *into* the
$X$ node being deleted. Therefore, we'd have the following two graphs:

```{dot fig_mutilated-graphs_simple-confounding}
//| fig-align: center
//| fig-cap: Mutilated graphs for simple confounding with three variables.
//| label: fig-mutil-graphs_simple-confounding

digraph {
    layout=dot;
    rankdir="TB";

    subgraph mutil_G_true {
        X_1 [label="X = x"];
        Z_1 [label="Z"];
        Y_1 [label="Y"];

        X_1 -> Y_1;
        Z_1 -> Y_1;
    }

    subgraph mutil_G_false {
        X_2 [label="X = x"];
        Z_2 [label="Z"];
        Y_2 [label="Y"];

        X_2 -> Y_2;
        Z_2 -> Y_2;
    }
}
```

So, in this case, we get the same graph. But do we get the same predictions?

```{julia}
#| echo: false

Markdown.parse("""
               Let's try setting \$X\$ to the mean of its observed values,
               $(mean(d.X) |> round_2) and see what predictions we'll get.
               """)
```

```{julia fig_compare_dag-bn-x-confound_mutilated}
pred_gen_model_y_xz = gen_model_y_xz(fill(mean(d.X),
                                          N),
                                     fill(missing,
                                          N),
                                     d.Z)

y_hat_dag = Turing.predict(pred_gen_model_y_xz,
                           chains)

pred_bn_model_y_xz_indep = bn_model_y_xz_indep(fill(mean(d.X),
                                                    N),
                                               fill(missing,
                                                    N),
                                               d.Z)

y_hat_bn = Turing.predict(pred_bn_model_y_xz_indep,
                          chains_xz_indep)

d_dag_bn = DataFrames.DataFrame("Y_dag" =>
                                    MCMCChains.summarystats(y_hat_dag)[:, :mean],
                                "Y_bn" =>
                                    MCMCChains.summarystats(y_hat_bn)[:, :mean])

compare_yhat(d_dag_bn)
```
