---
title: 'Exploring causal inference with DAGs and Bayesian models'
---

Lately, I've been reading quite a lot about causal inference in statistics and
the various approaches developed through the decades. Pearl's framework, which
relies on directed acyclic graphs (DAGs) resonated with me the most; I really
like DAGs as a way of expressing ideas about causality, and as a tool for
tackling causal questions and building causal models.

As I've been focused on Bayesian modeling for the past couple of years, I've
found the generative modeling approach to be quite a useful way of approaching
data analysis. The directed acyclic graph approach and the generative modeling
approach seem to have a lot in common. However, I still feel like a difference
is being made between the two. Because of this (and other pedagogic reasons),
I've decided to explore some simple causal setups to gain (hopefully) a better
understanding of both causal inference and the DAG/generative modeling
frameworks.

# Case 1 - three variables with simple confounding

The first case I'm going to look at is a simple three-variable case where one
variable acts as a confounder of the causal relationships of the remaining two
variables. I'm talking about the case pictured below:

```{dot}
//| fig-align: center
//| fig-cap: Three variables, simple confounding.
//| label: fig-three-var-confounding

digraph G {
    layout=dot;
    rankdir=LR;
    X -> Y;
    Z -> X;
    Z -> Y;
}
```

So, I'm assuming that the $X$ variable exerts a causal effect on the $Y$
variable, and that the $Z$ variable has a causal effect both on $X$ and $Y$.
Therefore, $Z$ creates a backdoor path from $X$ to $Y$, which means that it has
to be adjusted for in order to get an unbiased estimate of the causal effect of
$X$ on $Y$.

Before simulating some values for the variables, I'll load some Julia packages:

```{julia}
using Pkg

Pkg.activate("..")

using Distributions
using Random
using Turing
```

Now, let's simulate data from the data generating process described in
@fig-three-var-confounding. For simplicity, I'll assume all variables are
distributed normally with the same standard deviation $\sigma = 1$, but with
different means. Further, I'll assume that errors $\varepsilon$ are also
distributed normally with $\mu = 0$ and $\sigma = 1$. Also for simplicity, I'll
assume that the relationships between the variables can be described by a simple
linear model. Therefore, we have:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &\sim \text{normal}(\mu_{x_i}, 1) \\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{x_i} &= 3 z_i \\
    \mu_{y_i} &= 2 x_i + 4 z_i
\end{align}

```{julia}
X_distr = Distributions.Normal(1)
Y_distr = Distributions.Normal(2)
Z_distr = Distributions.Normal(3);
```

```{dot}
//| fig-cap: Causal effects for three variables, simple confounding scenario.
//| label: fig-three-var-confounding-effs
digraph effects_G {
    layout=dot;
    rankdir=LR;
    X -> Y [label="b_xy = 2"];
    Z -> X [label="b_zx = 3"];
    Z -> Y [label="b_zy = 4"];
}
```

With that in mind, I'll simulate the data:

```{julia}
N = 1e3
```
