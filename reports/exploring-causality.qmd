---
title: 'Exploring causal inference with DAGs and Bayesian models'
author: Denis Vlašiček
date: today
execute:
    cache: true
bibliography: references.bib
highlight-style: gruvbox-dark
format:
    html:
        toc: true
        toc-location: left
        theme: litera
        code-overflow: wrap
---

Lately, I've been reading quite a lot about causal inference in statistics and
the various approaches developed through the decades. Pearl's framework, which
relies on directed acyclic graphs (DAGs), resonated with me the most; I really
like DAGs as a way of expressing ideas about causality, and as a tool for
tackling causal questions and building causal models.

As I've been focused on Bayesian modeling for the past couple of years, I've
found the generative modeling approach to be quite a useful way of approaching
data analysis. The directed acyclic graph approach and the generative modeling
approach seem to have a lot in common. However, I still feel like a difference
is being made between the two. Because of this (and other pedagogic reasons),
I've decided to explore some simple causal setups to gain (hopefully) a better
understanding of both causal inference and the DAG/generative modeling
frameworks.

# Case 1 - three variables with simple confounding

The first case I'm going to look at is a simple three-variable case where one
variable acts as a confounder of the causal relationships of the remaining two
variables. We'll have a variable $X$ that exerts a causal influence on a
variable $Y$. We'll also have a variable $Z$ that has a causal effect on both
$X$ and $Y$, making $Z$ a confounder of the causal relationship between
$X$ and $Y$. This model can be represented by the following directed acyclic
graph (DAG), where each arrow represents the causal effect which the variable
at the arrow's tail has on the variable at the arrow's head (e.g. $X$ has a
causal effect on $Y$):

```{dot fig_dag_no-coef}
//| fig-align: center
//| fig-cap: Three variables, simple confounding.
//| label: fig-three-var-confounding

digraph G {
    layout=dot;
    rankdir=TB;
    X -> Y[minlen=4];
    Z -> X;
    Z -> Y;
    {rank="same"; X; Y;}
}
```

In the graph pictured in @fig-three-var-confounding
there are two paths between $X$ and $Y.$ The first path is the direct one, going
from $X$ to $Y$ ($X \textrightarrow Y$). The second path is an indirect one,
and goes over the $Z$ variable: $X \textleftarrow Z \textrightarrow Y$.

I've mentioned earlier that the direction of the arrows tells us the direction
of the causal effects. Therefore, we can say that the path
$X \textrightarrow Y$ is causal. However, the path
$X \textleftarrow Z \textrightarrow Y$ is not a causal path --- starting from
the left, we have an arrow going into $X$ from $Z$, and then an arrow going from
$Z$ into $Y$. Since this path starts with an arrow pointing *into* $X$, this
makes it a *backdoor path* between $X$ and $Y$.

The idea of backdoor paths is one of the central ideas in Pearl's framework.
Backdoor paths are not causal, but create problems when trying to estimate
causal effects. In the graph in @fig-three-var-confounding, we'd have to
somehow close the backdoor path in order to obtain an unbiased estimate of the
causal effect of $X$ on $Y$. In this case, the path can be close by adjusting
for the $Z$ variable. We'll see how to do that in a bit.

I'll first set up the Julia environment and simulate some values for the
variables $X$, $Z$ and $Y$.

```{julia setup}
#| output: false

using Pkg

Pkg.activate("..")

using Distributions
using Random
using Statistics
using StatsModels
using Turing
using GLM
using DataFrames
using Markdown
using StatsPlots
using QuadGK

include(joinpath("..",
                 "helpers",
                 "text-formatting.jl"))

include(joinpath("..",
                 "helpers",
                 "plots.jl"))

Random.seed!(1)
```

Now, let's simulate data from the data-generating process described in
@fig-three-var-confounding. For simplicity, I'll assume all variables are
distributed normally with the same standard deviation $\sigma = 1$, but with
different means. Also for simplicity, I'll assume that the relationships between
the variables can be described by a simple linear model. Therefore, we have:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &\sim \text{normal}(\mu_{x_i}, 1) \\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{x_i} &= 3 z_i \\
    \mu_{y_i} &= 2 x_i + 4 z_i,\; \text{for}\; i \in \{1, \ldots, N\}
\end{align}

```{dot fig_dag_effects}
//| fig-cap: Causal effects for three variables, simple confounding scenario.
//| label: fig-three-var-confounding-effs

digraph effects_G {
    layout=dot;
    bgcolor="transparent";
    rankdir=TB;
    X -> Y [label="b_XY = 2", minlen=4];
    Z -> X [label="b_ZX = 3"];
    Z -> Y [label="b_ZY = 4"];
    {rank="same"; X; Y;}
}
```

With that in mind, I'll simulate the data and put it in a data frame:

```{julia generate-data}
Z_distr = Distributions.Normal(1, 1);

N = 1000 # number of simulated draws

Z = Random.rand(Z_distr,
                N)

# create X values
x_mu = 3 * Z

X_distr = Distributions.Normal.(x_mu, 1)

X = Random.rand.(X_distr)

# create Y values
y_mu = 2 * X + 4 * Z

Y_distr = Distributions.Normal.(y_mu, 1)

Y = Random.rand.(Y_distr)

d = DataFrames.DataFrame("Y" => Y,
                         "X" => X,
                         "Z" => Z)

d[1:10, :]
```

## Linear regression and causal DAGs

I've mentioned earlier that we have to somehow close the backdoor path over $Z$
if we want to get an unbiased estimate of the causal effect of $X$ on $Y$.
Since we're dealing with simple linear models, we'll close the backdoor path by
including $Z$ as a predictor in a multiple linear regression model, as this will
adjust for its effect.

But first, let's fit the model without including $Z$ to see what estimate of the
causal effect of $X$ on $Y$ we'll get. When simulating data, we've set the $b$
coefficient for the effect of $X$ on $Y$ to $b_{XY} = 2$, so what we'd expect
is an estimate $\hat{b}_{XY} \ne 2$.

(Note: I'm naming the coefficients so that the left variable in the subscript
refers to the variable causing the change, and the right variable in the
subscript refers to the variable being changed. Therefore, $b_{XY}$ denotes the
coefficient which represents our belief about the causal effect of $X$ on
$Y$.)

```{julia fit_confounded}
f = StatsModels.@formula(Y ~ X)

fit_y_x = GLM.lm(f,
                 d)
```

```{julia}
#| echo: false

x_idx = findall(x -> x == "X",
                coefnames(fit_y_x))[1]

Markdown.parse("""
               As can be seen, we get
               \$\\hat{b}_{XY} = $(coef(fit_y_x)[x_idx] |> round_2)\$, which is
               different than the value set in the simulation.
               """)
```

As mentioned earlier, we should be able to get an unbiased estimate of the
effect of $X$ on $Y$ if we control for the confounder $Z$. Therefore, let's try
fitting another model, this time including $Z$ as a variable in the regression
model alongside $X$.

```{julia fit_non-confounded}
f = StatsModels.@formula(Y ~ X + Z)

fit_y_xz = GLM.lm(f,
                  d)
```

Now we get a $\hat{b}_{XY}$ coefficient which corresponds to the one we've coded
up while simulating the data. The estimate of the effect of $Z$ on $Y$ is also
in line with the value which we've coded up.

## Bayesian networks, generative models and whatnot

Okay, now to the part that's a bit more tricky. I want to explore the
relationships between Bayesian networks and causal directed acyclic graphs.
If my understanding is correct, a Bayesian network is, in a way, a subset of a
causal directed acyclic graph --- it encodes the relationships between the
variables under investigation, but doesn't make additional assumptions which are
necessary for causal inference. However, a Bayesian network that corresponds to
the true DAG should be able to provide us with unbiased estimates of the causal
effects. A Bayesian network that does not correspond to the true DAG, on the
other hand, should provide biased estimates of the causal effects, but should
still be able to make accurate predictions. This may be a fine distinction, and
is something I also aim to explore further down.

I'll fit the model represented in @fig-three-var-confounding-effs
using the `Turing.jl` library. The model can be coded as follows:

```{julia model_three-var-simple-confound}
#| results: false
Turing.@model function gen_model_y_xz(X,
                                      Y,
                                      Z)
    s_X ~ Turing.Exponential(1)
    s_Y ~ Turing.Exponential(1)

    mu_z ~ Turing.Normal(2, 1)
    s_Z ~ Turing.Exponential(1)

    Z ~ Turing.Normal(mu_z, s_Z)

    b_ZX ~ Turing.Normal(2, 1)
    b_XY ~ Turing.Normal(2, 1)
    b_ZY ~ Turing.Normal(2, 1)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_ZX,
                             s_X)

        Y[i] ~ Turing.Normal(X[i] * b_XY + Z[i] * b_ZY,
                             s_Y)
    end
end
```

Next, we use the NUTS sampler (the same one *Stan* is using) to get samples from
the posterior distribution:

```{julia sample_three-var-simple-confound}
#| warning: false
#| message: false
chains = Turing.sample(gen_model_y_xz(d.X,
                                      d.Y,
                                      d.Z),
                       Turing.NUTS(1000,
                                   0.80),
                       Turing.MCMCThreads(),
                       3000,
                       4)

MCMCChains.summarystats(chains)
```

Indeed, the parameters estimated in this way correspond to the ones we've
simulated earlier. Now, this probably isn't that surprising, given that our
model is coded in line with the true DAG. Still, that *is* comforting, I guess.

I'll try repeating the analysis using a distribution for the $Z$ variable which
I know to be incorrect - the Cauchy distribution with location 3 and scale 1.

```{julia model_three-var-simple-confound_cauchy}
Turing.@model function gen_model_y_xz_cauchy(X,
                                             Y,
                                             Z)
    s_X ~ Turing.Exponential(1)
    s_Y ~ Turing.Exponential(1)

    b_ZX ~ Turing.Normal(2, 1)
    b_XY ~ Turing.Normal(2, 1)
    b_ZY ~ Turing.Normal(2, 1)

    Z ~ Turing.Cauchy(3, 1)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_ZX,
                             s_X)

        Y[i] ~ Turing.Normal(X[i] * b_XY + Z[i] * b_ZY,
                             s_Y)
    end
end
```

```{julia sample_three-var-simple-confound_cauchy}
#| warning: false
#| message: false
chains_cauchy = Turing.sample(gen_model_y_xz_cauchy(d.X,
                                                    d.Y,
                                                    d.Z),
                              Turing.NUTS(1000,
                                          0.80),
                              Turing.MCMCThreads(),
                              3000,
                              4)

MCMCChains.summarystats(chains_cauchy)
```

Luckily, this also seems to work. So we can be somewhat off.

Now, let's examine a DAG we know to be false, and compare the predictions of a
model that's coded in line with that DAG, with those of the model coded in line
with the true DAG. If my understanding is correct, both the causal DAG and the
Bayesian network should be able to make predictions of similar accuracy.
Moreover, they should make (practically) equivalent predictions. That
would mean that values $\hat{y}^{cDAG}$ predicted from the causal DAG should be
(practically )identical to the values $\hat{y}^{BN}$ predicted by a Bayesian
network.

To check this claim, I'll try modeling the data according to the following
graph, which we know to be false, as here we have a causal effect of $X$ on
$Z$, and not the other way around:

```{dot fig_bn_x-confound}
//| fig-align: center
//| fig-cap: The obviously false DAG.
//| label: fig-three-var-confounding-x-confound

digraph G {
    layout=dot;
    Z[rank="min"];
    rankdir=TB;
    X -> Y[minlen=4];
    X -> Z[color="red"];
    Z -> Y;
    {rank="same"; X; Y;}
}
```

This graph implies a different set of equations representing the relationships
between the variables[^mu_x]:

\begin{align}
    X &\sim \text{normal}(\mu_X, \sigma_X) \\
    z_i &\sim \text{normal}(\mu_{z_i}, \sigma_Z) \\
    y_i &\sim \text{normal}(\mu_{y_i}, \sigma_Y) \\
    \mu_{z_i} &= b_{XZ} x_i \\
    \mu_{y_i} &= b_{XY} x_i + b_{ZY} z_i,\; \text{for}\; i \in \{1, \ldots, N\}
\end{align}

I'll again code and fit the model using `Turing.jl`:

```{julia model_bn_x-confound}
#| warning: false
#| message: false
Turing.@model function bn_model_y_xz(X,
                                     Y,
                                     Z)
    mu_X ~ Turing.Normal(2, 1)
    s_X ~ Turing.Exponential(1)

    X ~ Turing.Normal(mu_X, s_X)

    s_Y ~ Turing.Exponential(1)
    s_Z ~ Turing.Exponential(1)

    b_XZ ~ Turing.Normal(2, 1)
    b_XY ~ Turing.Normal(2, 1)
    b_ZY ~ Turing.Normal(2, 1)

    for i in eachindex(X)
        Z[i] ~ Turing.Normal(X[i] * b_XZ,
                             s_Z)

        Y[i] ~ Turing.Normal(X[i] * b_XY + Z[i] * b_ZY,
                             s_Y)
    end
end

chains_x_conf = Turing.sample(bn_model_y_xz(d.X,
                                            d.Y,
                                            d.Z),
                              Turing.NUTS(1000,
                                          0.80),
                              Turing.MCMCThreads(),
                              3000,
                              4)

MCMCChains.summarystats(chains_x_conf)
```

So, the estimates for $b_{XY}$ and $b_{ZY}$ are actually quite fine.
The coefficient for the effect of $X$ on $Z$ ($b_{XZ}$) is, of course,
gibberish, if interpreted causally. But how do the predicted $Y$ values from
this model compare to the values predicted from the true DAG?

Let's compare each of these models' $\hat{y}$ values to the observed $Y$ values,
as well as the models' predictions to each other. I'll use only the mean
estimates of the parameters, disregarding the uncertainty.

```{julia fig_compare_dag-bn-x-confound}
pred_gen_model_y_xz = gen_model_y_xz(d.X,
                                     fill(missing,
                                          N),
                                     d.Z)

y_hat_dag = Turing.predict(pred_gen_model_y_xz,
                           chains)

pred_bn_model_y_xz = bn_model_y_xz(d.X,
                                   fill(missing,
                                        N),
                                   d.Z)

y_hat_bn = Turing.predict(pred_bn_model_y_xz,
                          chains_x_conf)

d_dag_bn = DataFrames.DataFrame("Y_dag" =>
                                    MCMCChains.summarystats(y_hat_dag)[:, :mean],
                                "Y_bn" =>
                                    MCMCChains.summarystats(y_hat_bn)[:, :mean],
                                "Y_obs" => Y)

p_dag_obs = compare_yhat(d_dag_bn;
                         y_pred_1_name = "Y_dag",
                         y_pred_2_name = "Y_obs")

p_bn_obs = compare_yhat(d_dag_bn;
                        y_pred_1_name = "Y_bn",
                        y_pred_2_name = "Y_obs")

p_dag_bn = compare_yhat(d_dag_bn)

plot(p_dag_obs,
     p_bn_obs,
     p_dag_bn,
     layout = (3, 1))
```

Aside from seemingly being very good at predicting the $Y$ values, both models
seem to make, for all practical intents and purposes, identical predictions.

## Predicting the effects of interventions

But what does this all mean for predicting the effects of interventions, as one
would do using Pearl's *do*-calculus, which assumes knowledge of the causal
 mechanism? According to
@pearlCausalInferenceStatistics2016 an intervention can be represented in a DAG
by removing all arrows going into the intervened upon variable.
For example, here, we'll try predicting the effect of an intervention
that would set the value of $X$ to some $x$, denoted $do(X = x)$.
Under such an intervention, our two competing DAGs would look like this:

```{dot fig_compare-dags}
//| fig-align: center
//| fig-cap: Causal DAGs when we simulate a $do(X = x)$ intervention. True DAG left, wrong DAG right.
//| label: fig-simple-confounding-do-compare

digraph G {
    layout=dot;
    subgraph dag_true {
        rankdir=TB;
        X1[label="X"];
        Y1[label="Y"];
        Z1[label="Z"];
        X1 -> Y1[minlen=4];
        Z1 -> X1[style="invisible", arrowhead="none"];
        Z1 -> Y1;
        {rank="same"; X1; Y1;}
    }
    subgraph dag_false {
        rankdir=TB;
        X2[label="X"];
        Y2[label="Y"];
        Z2[label="Z"];
        X2 -> Y2[minlen=4];
        Z2 -> Y2;
        X2 -> Z2;
        {rank="same"; X2; Y2;}
    }
    {rank="same"; Z1; Z2;}
}
```

So, we could ask, for example, what is the expected value of the $Y$ variable if
we were to conduct an intervention that would set the value of $X$ to 6. We are
interested in $\text{E}(Y | do(X = 6))$.

Given that we know the equations of the true-data generating process, we can
calculate this. We know that each $y_i$ value is a draw from a normal
distribution with location $\mu_{y_i}$ and scale $1$. We further know that the
value of $\mu_{y_i}$ is determined by the values which the variables $X$ and $Z$
take for a chosen $i$ (i.e. the values $X = x_i$ and $Z = z_i$).

Since we've made the intervention $do(X = 6)$, our first step will be to replace
$x_i$ with $6$ --- imagine that, somehow, we can intervene into reality and set
$X = 6$ for each observation. Since the values of the $Z$ variable also
influence the values of $Y$, we'll average over the $Z$ values.

This was our starting model specification:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &\sim \text{normal}(\mu_{x_i}, 1) \\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{x_i} &= 3 z_i \\
    \mu_{y_i} &= 2 x_i + 4 z_i
\end{align}

We'll replace the second line with a constant:

\begin{align}
    Z &\sim \text{normal}(1, 1) \\
    x_i &= 6\; \text{for all}\; i\\
    y_i &\sim \text{normal}(\mu_{y_i}, 1) \\
    \mu_{y_i} &= 2 x_i + 4 z_i \\
\end{align}

We've said earlier that the expected value of $Y$ given some values of $X$ and
$Z$ is equal to $\mu_{y_i}$:

\begin{equation}
    \mu_{y_i} = \text{E}(Y | X, Z).
\end{equation}

Next, given that we've set $X = 6$ through an intervention (and thus replaced
$x_i \sim \text{normal}(\mu_{x_i}, 1)$ with a constant), we have:

\begin{equation}
    \text{E}(Y | X = 6, Z) = 2 \times 6 + 4 z_i
\end{equation}

Finally, we want to average over the values of $Z$ to get $\text{E}(Y | X)$:

\begin{align}
    \text{E}(Y | X) &= \text{E}(\text{E}(Y | X, Z)) \\
                    &= \int_{z \in Z}  \text{E}(Y | X = 6, Z = z) f_Z(z)\;
                        \text{d}z \\
                    &= \int_{z \in Z}(2 \times 6 + 4z)f_Z(z)\, \text{d}z.
\end{align}

We can use the `QuadGK.jl` package to calculate this expectation. Here, we'll
rely on the fact that we know the distribution of $Z$, as we're looking for
the effect of the intervention according to the true data-generating model.

```{julia calculate-true-effect-integral}
function integral(z)
    f_Z = Distributions.Normal(1, 1)
    return (2 * 6 + 4 * z) * Distributions.pdf(f_Z, z)
end

QuadGK.quadgk(integral,
              -Inf,
              Inf)
```

To confirm that our calculation is correct, we'll also simulate a new set of $Y$
values, with $X = 6$:

```{julia true-effect-simulate}
N_sim = 100_000

x_i = 6

Z_prime = Random.rand(Z_distr,
                      N_sim)

# create Y values
y_mu = 2 * x_i .+ 4 * Z_prime

Y_x = Distributions.Normal.(y_mu, 1)

Y_x_fixed = Random.rand.(Y_x)

mean(Y_x_fixed)
```

As we can see, we're getting a very similar result. This gives us additional
confidence that our calculation was correct.

### Interventions according to the true DAG

Now, what's the deal with the two models --- how do we obtain $\text{E}(Y | do(X
= 6))$? Let's first look at the model corresponding to the true
data-generating process.

Our first step is to get from the $\text{E}(Y | do(X = 6))$ expression to
$\text{E}(Y | X = 6)$. The $do(\cdot)$ operator has special meaning in Pearl's
framework --- it represents conducting an intervention that sets the value of
one (or more) variables to some specific value. In our case, $do(X = 6)$ means
conducting an intervention that sets all values of $X$ to 6. Therefore,
within Pearl's framework there's a difference between $\text{E}(Y | do(X = 6))$
--- which represents the expected value of the $Y$ variable if we were to
*intervene* on $X$ so that all values of $X$ are set to 6 --- and
$\text{E}(Y | X = 6)$ --- which is simply the expected value of $Y$ we *observe*
for cases where $X = 6$.

@pearlCausalCalculusStatistical1996 introduces a set of three rules, called the
do-calculus, which allows us to transition from $\text{E}(Y | do(X = 6))$
to $\text{E}(Y | X = 6)$ if it's possible given the DAG we're relying
on.[^intervention_notation] According to these rules, we start with
$\text{E}(Y | do(X = 6), Z = z)$ --- according to our DAG, $Y$ depends both
on $X$ and $Z$; it's conditional on both. Furthermore, since we're doing an
intervention on $X$ we have $do(X = 6)$; since we're only observing the value of
$Z$, there's no $do(Z = z)$ term, but only $Z = z$.

Since our goal is $\text{E}(Y | X = 6)$, the first thing we'll try doing is
converting the $do(X = 6)$ into $X = 6$ --- this would allow us to think about
the effect of the intervention $do(X = 6)$ based on data which are not
interventional (i.e. we haven't actually collected data after making an
intervention), but observational (we have just observed the world in a certain
state). We're allowed to do this if $X$ is independent
of $Y$, conditional on $Z$, in a modified DAG where all arrows exiting $X$ are
removed. This may be written as $(Y \perp X | Z)_{G_{\underline{X}}}$, where
$G_{\underline{X}}$ stands for "graph where all arrows going out of $X$ are
removed". This graph is shown in @fig-three-var-confounding-x-out-mutil.

```{dot fig_dag-true_from-x-removed}
//| fig-cap: Graph with all arrows emanating from $X$ removed.
//| label: fig-three-var-confounding-x-out-mutil

digraph effects_G {
    layout=dot;
    bgcolor="transparent";
    rankdir=TB;
    X -> Y[minlen=4; style="invisible"; arrowhead="none"];
    Z -> X;
    Z -> Y;
    {rank="same"; X; Y;}
}
```

As we can seen, once we've removed the arrow $X \textrightarrow Y$, and
conditioning on $Z$, $X$ and $Y$ really are independent. Therefore, we can
replace $\text{E}(Y | do(X = 6), Z)$ with $\text{E}(Y | X = 6, Z)$. The final
step is adjusting for $Z$, since it's confounding the causal relationship
between $X$ and $Y$ [@pearlCausalInferenceStatistics2016].
We do that by integrating over the values of $Z$, which
leads to a set of equations analogous to the one we've had when looking at the
expectation from the true data-generating process. However, we'll replace the
hard-coded, true coefficient values, with our estimates:

\begin{align}
    \text{E}(Y | do(X = 6), Z) &= \text{E}(Y | X = 6, Z) \\
        &= \text{E}(\text{E}(Y | X = 6)) \\
        &= \int_{z \in Z}\text{E}(Y | X = 6, z) f_Z(z)\,
            \text{d}z \\
        &= \int_{z \in Z}(\hat{b}_{YX} \times 6 +
            \hat{b}_{YZ} \times z)f_Z(z)\, \text{d}z.
\end{align}

```{julia}
#| echo: false
Markdown.parse("""Let's take the posterior means of the parameter estimates
               for \$\\hat{b}_{XY}\$ and \$\\hat{b}_{ZY}\$, that is
               \$b_{XY} =
               $(MCMCChains.summarystats(chains)[6, :mean] |> round_2)\$
               and \$b_{ZY} =
               $(MCMCChains.summarystats(chains)[7, :mean] |> round_2)\$""")
```

```{julia true-dag-simulate-intervention}
mu_Z = MCMCChains.summarystats(chains)[3, :mean]
s_Z = MCMCChains.summarystats(chains)[4, :mean]

b_xy = MCMCChains.summarystats(chains)[6, :mean]
b_zy = MCMCChains.summarystats(chains)[7, :mean]

function integral(z)
    f_Z = Distributions.Normal(mu_Z, s_Z)
    return (b_xy * 6 + b_zy * z) * Distributions.pdf(f_Z, z)
end

QuadGK.quadgk(integral,
              -Inf,
              Inf)
```

Alternatively, we could do:

```{julia true-dag-calculate-simulated}
1 / N * sum(b_xy * 6 .+ b_zy * Z)
```

The results align almost perfectly with the one obtained from the true model,
which could have been expected, given that we've estimated both $b_{XY}$ and
$b_{ZY}$ approximately correctly. So we basically just reran the previous
calculation.

### Interventions according to the wrong DAG

Now for the second model. As we've mentioned earlier, no arrows get deleted
in this model, as there are no arrows entering $X$.

```{dot fig_wrong-dag_intervention}
//| fig-align: center
//| fig-cap: DAG of the intervention $do(X = 6)$ in the wrong DAG.
//| label: fig-simple-confounding-wrong-dag-interv

digraph G {
    layout=dot;
    rankdir=TB;
    Z[label="Z"];
    X[label="X"];
    Y[label="Y"];
    Z -> Y;
    X -> Z;
    X -> Y[minlen=4];
    {rank="same"; X; Y;}
}
```

To get the total causal effect of $do(X = 6)$ on $Y$, we won't adjust for $Z$
because that would close the indirect, *causal* path $X \textrightarrow Z
\textrightarrow Y$. If we were to adjust for $Z$, we'd close off a part of the
effect of the intervention, given that --- according to this DAG --- the value
of $X$ also influences the value $Z$ will take on.

Keeping that in mind, let's again apply the rules of do-calculus to see if, and
how, we can get from $\text{E}(Y | do(X = 6, Z))$ to $\text{E}(Y | X = 6, Z)$.
Again, our first step will be to try to turn $do(X = 6)$ into $X = 6$. According
to the rules, we can do that if $X$ is independent of $Y$ when all of the arrows
emanating from $X$ are removed from the graph (and, possibly, while adjusting
for $Z$). The mutilated graph looks like this:

```{dot fig_wrong-dag_from-x-removed}
//| fig-align: center
//| fig-cap: Wrong DAG with arrows leaving $X$ removed.
//| label: fig-simple-confounding-wrong-dag-interv-mutil

digraph G {
    layout=dot;
    rankdir=TB;
    Z[label="Z"];
    X[label="X"];
    Y[label="Y"];
    Z -> Y;
    X -> Z[style="invisible"; arrowhead="none"];
    X -> Y[minlen=4; style="invisible"; arrowhead="none"];
    {rank="same"; X; Y;}
}
```

We see that, even without adjusting for $Z$, $X$ and $Y$ are in no way connected
in this graph; there are no arrows connecting $X$ and $Y$ either directly or
indirectly. Therefore, we conclude that $X$ and $Y$ are independent in such a
graph ($(Y \perp X)_{G_{\underline{X}}}$), and so it is justified to replace
$do(X = 6)$ with $X = 6$ in our expression.

Can we remove $Z$ from $\text{E}(Y | X = 6, Z)$? Seemingly not. $Z$ would have
to be independent of $Y$ in our original graph, which we cannot achieve, even if
we were to condition on $X$ because $Z$ and $Y$ are directly related. So we,
again, end up with $\text{E}(Y | X = 6, Z)$.

However, as mentioned earlier, we won't be adjusting for $Z$ because its value
is influenced by the value of $X$; if $X$ is set to some value $x$ through an
intervention, $Z$ also changes. We have to keep that in mind if we want to get
an estimate of the total causal effect $X$ has on $Y$. So we'll modify our
expression, and mark the new variable as $Z'$, keeping in mind that it
represents the values that the $Z$ variable would obtain if the $X$ values were
set to some $x$ (in our case, 6):[^counterfactuals]
$\text{E}(Y | do(X = 6), Z')$. According to our model,

\begin{align}
    \text{E}(Y | do(X = 6), Z') &= \text{E}(Y | X = 6, Z') \\
        &= b{XY} \times 6 + b{ZY} \times Z'.
\end{align}

To obtain an estimate of $\text{E}(Y | do(X = 6), Z)$ we can take our posterior
estimates of $b_{XY}$ and $b_{ZY}$, simulate a
set of $Z'$ values, and calculate

$$
    \frac{1}{N} \sum_{i}^N (\hat{b}_{XY} \times 6 +
        \hat{b}_{ZY} \times z'_i)
$$

```{julia calculate-te-simulation}
s_Z = MCMCChains.summarystats(chains_x_conf)[4, :mean]

b_xz = MCMCChains.summarystats(chains_x_conf)[5, :mean]
b_xy = MCMCChains.summarystats(chains_x_conf)[6, :mean]
b_zy = MCMCChains.summarystats(chains_x_conf)[7, :mean]

Z_prime_distr = Distributions.Normal(b_xz * 6, s_Z)

Z_prime = Random.rand(Z_prime_distr,
                      10_000)

mean(b_xy * 6 .+ b_zy * Z_prime)
```

Alternatively, if we rearrange the previous equation, we'll notice that we're
really working with the expected value of the $Z'$ variable:

\begin{align}
    \text{E}(Y | X = 6, Z') &= \frac{1}{N} \sum_{i}^N (\hat{b}_{XY} \times 6
        + \hat{b}_{ZY} \times z'_i) \\
    &= \hat{b}_{XY} \times 6 + \hat{b}_{ZY} \times
        \underbrace{\frac{1}{N} \sum_i^N z'_i}_{\text{E(Z')}}.
\end{align}

According to our model, that is:

$$
    \text{E}(Z') = \mu_{z'_i} = b_{XZ} \times x_i,\; \text{where}\;
        x_i = 6\; \forall\, i
$$

meaning that we can also estimate the total causal effect of $do(X = x)$ on
$Y$ like this:

```{julia calculate-te-short}
#                   E(Z')
b_xy * 6 + b_zy * (b_xz * 6)
```

Here, we get an estimate of the total causal effect of $X$ on $Y$ that's clearly
biased upwards.

#### Direct and indirect effects

Now, I've been trying to make it clear that, for the wrong DAG, we're talking
about the total causal effect of setting $do(X = 6)$ on the expected value of
$Y$. This was because, according to the models, $X$ affects $Y$ both directly
and indirectly, by changing the value of $Z$. Hence, the total effect of setting
$do(X = 6)$ can be broken down into a (natural) direct effect, and a (natural)
indirect effect[^natural_effect]. The natural direct effect refers to the
expected value of the $Y$ variable when $X$ is set to some value, and the $Z$
variable is set to whatever value it would have had prior to setting $X =
x$ [@pearlCausalInferenceStatistics2016]
[^natural_direct_eff].

We'll use the value $X = 0$ as the reference point. In other words, when saying
that the variable $Z$ holds values which $Z$ would have attained *prior to
setting* $X = 6$, we're talking about the values $Z$ would have attained had
$X$ been 0.

How would we go about calculating the natural direct effect? The explanation is
a bit long, and requires going into a lot of things not covered here (but see
materials listed in the previous footnote). However, I'll try giving a brief
explanation. In our wrong DAG, the $Z$ variable serves as a link between $X$ and
$Y$. Since the path $X \textrightarrow Z \textrightarrow Y$ is causal, these
three variables form a chain, where $Z$ is a mediator of the effect of $X$ on
$Y$. In order to be able to identify the natural direct effect, there has to
exist a set of measured variables, let's call it $W$, that satisfies two
conditions. First, no variable in $W$ is allowed to be a descendant of $X$; in
other words, $W$ must not have an arrow going into it from $X$, or from any
variable that has an arrow out of $X$ going into it. Second, the variables in
$W$ have to block all backdoor paths from the mediator $Z$ to $Y$, once we've
removed the arrows going from $X$ to $Z$ and from $Z$ to $Y$. Since our DAG is
really simple, the empty set (i.e. containing no variables), satisfies these two
conditions; our set $W$ is empty. Therefore, we can get the natural direct
effect as:

$$
    NDE = \int_z [\text{E}(Y | do(X = 6, Z = z)) -
        \text{E}(Y | do(X = 0, Z = z))] \times p(Z = z | do(X = 0)).
$$

Let's break this down by parts. First up is the expectation
$\text{E}(Y | do(X = 6, Z = z))$. Here, we apply the do-operator both to $X$ and
$Z$. Can we get from the $do(\cdot)$ expression to a simple observation?
First, let's try turning $do(Z = z)$ into $Z = z$. Given that we're also
intervening on $X$, $Z$ has to be independent from $Y$ in a DAG where all arrows
entering $X$ and all arrows exiting $Z$ are removed (and possibly after
conditioning on $X$):

```{dot fig_wrong-dat_from-z-into-x-removed}
//| fig-align: center
//| fig-cap: Wrong DAG with arrows leaving $Z$ and entering $X$ removed.
//| label: fig-simple-confounding-wrong-dag-interv-xz

digraph G {
    layout=dot;
    rankdir=TB;
    Z[label="Z"];
    X[label="X"];
    Y[label="Y"];
    Z -> Y[style="invisible"; arrowhead="none"];
    X -> Z;
    X -> Y[minlen=4];
    {rank="same"; X; Y;}
}
```

This really is the case, conditional on $X$, $Z$ and $Y$ are independent in such
a graph: $(Z \perp Y | X)_{G_{\bar{X}\underline{Z}}}$. Therefore, we can write
$\text{E}(Y | do(X = 6), do(Z = z)) = \text{E}(Y | do(X = 6), Z = z)$. Next, we
have to get rid of $do(X = 6)$. We know we can do that from earlier, when we
were calculating the total effect of intervening on $X$ on $Y$. Therefore,
we know that $\text{E}(Y | do(X = 6, Z = z)) = \text{E}(Y | X = 6, Z = z)$, and
also that $\text{E}(Y | do(X = 0, Z = z)) = \text{E}(Y | X = 0, Z = z)$.

Next, we have $p(Z = z | do(X = 0))$. In order to replace the do-operator with a
simple observation, $Z$ and $X$ have to be independent in a graph where all
arrows exiting $X$ have been removed. We have this graph in
@fig-simple-confounding-wrong-dag-interv-mutil. This is also the case.
Therefore, we can replace the do-operator with an observation. This means that
our natural direct effect can be calculated as:

$$
    NDE = \int_z [\text{E}(Y | X = 6, Z = z) -
        \text{E}(Y | X = 0, Z = z)] \times p(Z = z | X = 0).
$$

According to our model specification:

\begin{align}
    \text{E}(Y | X = x, Z = z) &= \mu_{y_i} \\
        &= b_{XY} x_i + b_{ZY} z_i \\
    p(Z = z | X = 0) &= \text{normal}(Z = z | \mu_{z_i}, \sigma_Z) \\
        &= \text{normal}(Z = z | b_{XZ} x_i, \sigma_Z)
\end{align}

We can, again, code this using `QuadGK`, plugging in our posterior estimates of
the parameter values:

```{julia calculate-nde-integral}
function integral(z)
    f_Z = Distributions.Normal(b_xz * 0, s_Z)
    out = ((b_xy * 6 + b_zy * z) - (b_xy * 0 + b_zy * z)) *
        Distributions.pdf(f_Z, z)
end

QuadGK.quadgk(integral,
              -Inf,
              Inf)
```

Alternatively, for example, we could simulate data corresponding to the
given definition of the natural direct effect:

```{julia calculate-nde-simulation}
# distribution of Z when X is set to 0
Z_0_distr = Distributions.Normal(b_xz * 0, s_Z)

# random draws from the distribution of Z when X is set to 0
Z_0 = Random.rand(Z_0_distr,
                  10_000)

#= distributions corresponding to the combinations of X = {0, 6} and various
simulated Z values =#
Y_0_distr = Distributions.Normal.(b_xy * 0 .+ Z_0)
Y_6_distr = Distributions.Normal.(b_xy * 6 .+ Z_0)

# draws 50 values from each distribution
Y_0 = Random.rand.(Y_0_distr,
                   50)
Y_6 = Random.rand.(Y_6_distr,
                   50)

# get mean for each set of draws
Y_0 = map(mean,
          Y_0)
Y_6 = map(mean,
          Y_6)

# mean of the difference
nde = mean(Y_6 .- Y_0)
```

#### Natural indirect effect

The natural indirect effect refers to the expected value of $Y$ while $X$ is set
to some value $x$, and $Z$ takes the value it would have had if $X$ were set to
some $x'$. We'll again use $X = 0$ as a reference point.

In the case of our DAG, the natural indirect effect may be calculated as

$$
    NIE = \int_z \text{E}(Y | X = 0, Z = z)[p(Z = z | X = 6) -
        p(Z = z | X = 0)]
$$

Let's try computing this.

```{julia calculate-nie}
function integral(z)
    f_Z_0 = Distributions.Normal(b_xz * 0, s_Z)
    f_Z_6 = Distributions.Normal(b_xz * 6, s_Z)

    return (b_xy * 0 + b_zy * z) *
        (Distributions.pdf(f_Z_6, z) - Distributions.pdf(f_Z_0, z))
end

nie = QuadGK.quadgk(integral,
                    -Inf,
                    Inf)
```

As I've mentioned earlier, the total causal effect can be *decomposed* into the
natural direct effect and the natural indirect effect. Therefore, if our
calculations are correct, we'd expect that the sum of our natural indirect and
direct effects (approximately) corresponds to the total effect we've obtained
earlier. This really is the case with our calculations:

```{julia sum-nde-nie}
nde + nie[1]
```

## A less scary way to compute?

Now, we've seemingly managed to identify and calculate the effect an
intervention on $X$ would have on $Y$ when assuming different causal
relationships between variables. Until now, we've relied on integrals and
simulations to calculate the effects (and have only dealt with point estimates).
This wasn't necessarily very complicated, but also wasn't simple and
straightforward.

There's an interesting text by @lattimoreCausalInferenceBayes2019, where they
claim that we can get estimates of causal effects using the Bayesian machinery,
without having to use do-calculus at all. The idea is that we simultaneously fit
a pre-intervention and post-intervention model. Sounds esoteric, but is
relatively simple in practice.

### Interventions according to the true DAG. 

Let's return to the model which is in line with the true directed acyclic graph
(DAG). We've already mentioned that interventions can be represented by removing
all arrows going into the variable that's being intervened upon.
@lattimoreCausalInferenceBayes2019 take this idea an model the pre-intervention
and post-intervention states simultaneously. A key step is to connect all
variables which are not being intervened upon with a common, latent parameter.
I'll label those parameters $\theta$, and subscript them with the variable
they're referring to. For example, $\theta_X$ would be the parameter connecting
the pre-intervention variable $X$ with the post-intervention variable, which
I'll label $X'$.

So, keeping all that in mind, what we're trying to model is shown in
@fig-simple-confounding-bayes-intervention-true.

```{dot fig_true-dag_bayes-model}
//| fig-align: center
//| fig-cap: Graph with pre-intervention model on the left and post-intervention model on the right.
//| label: fig-simple-confounding-bayes-intervention-true

digraph G {
    layout=dot;
    rankdir=TB;
    theta_Z[label=<&theta;<SUB>Z</SUB>>];
    theta_Z -> {Z1; Z2}[style="dashed"];
    theta_Y[label=<&theta;<SUB>Y</SUB>>];
    theta_Y -> {Y1; Y2}[style="dashed"];
    {rank="same"; theta_Z; theta_Y}
    subgraph pre {
        pencolor="none";
        X1[label=<X<SUB>n</SUB>>];
        Y1[label=<Y<SUB>n</SUB>>];
        Z1[label=<Z<SUB>n</SUB>>];
        X1 -> Y1[minlen=4];
        Z1 -> X1
        Z1 -> Y1;
        {rank="same"; X1; Y1}
    }
    subgraph post {
        pencolor="none";
        X2[label="X'"];
        Y2[label="Y'"];
        Z2[label="Z'"];
        X2 -> Y2[minlen=4];
        Z2 -> Y2;
        Z2 -> X2[style="invisible"; arrowhead="none"];
        {rank="same"; X2; Y2}
    }
}
```

Let's fit this model in `Turing.jl`:

```{julia}
Turing.@model function simul_model(X,
                                   Y,
                                   Z)
    mu_Z ~ Turing.Normal(2, 1)
    s_Z ~ Turing.Exponential(1)

    Z ~ Turing.Normal(mu_Z, s_Z)

    mu_Y ~ Turing.Normal(2, 1)
    s_Y ~ Turing.Exponential(1)

    s_X ~ Turing.Exponential(1)

    b_ZX ~ Turing.Normal(2, 1)
    b_XY ~ Turing.Normal(2, 1)
    b_ZY ~ Turing.Normal(2, 1)

    for i in eachindex(X)
        X[i] ~ Turing.Normal(Z[i] * b_ZX,
                             s_X)

        Y[i] ~ Turing.Normal(X[i] * b_XY + Z[i] * b_ZY,
                             s_Y)
    end

    Z_prime ~ Turing.Normal(mu_Z, s_Z)

    X_prime = 6

    Y_prime_distr = Turing.Normal(X_prime * b_XY + Z_prime * b_ZY,
                                  s_Y)

    Y_prime = Random.rand(Y_prime_distr, 50)

    return Y_prime
end

chains_simul = Turing.sample(simul_model(X,
                                         Y,
                                         Z),
                             Turing.NUTS(1000,
                                         0.80),
                             Turing.MCMCThreads(),
                             1000,
                             4)

Y_prime = Turing.generated_quantities(simul_model(X,
                                                  Y,
                                                  Z),
                                      chains_simul)

Y_prime_exp = map(mean,
                  Y_prime) |>
    vec

mean(Y_prime_exp) |>
    display

quantile(Y_prime_exp,
         [0.1, 0.9]) |>
    display
```

First, I've coded the model using `Turing.jl`. After sampling from the joint
model, I've asked `Turing` to generate quantities --- i.e. `Y_prime` --- from
the model. For each iteration of the MCMC, `Y_prime` holds 50 samples from the
distribution of $Y'$. After extracting the `Y_prime` values, I've calculated the
mean of each sample of size 50, and then the mean of those means. What we get is
suspiciously similar to the expected value we've calculated in 
@interventions-acccording-to-the-true-dag. This seems nice. However, as a sanity
check, let's also try calculating the values implied by the wrong DAG.

[^counterfactuals]: Hello, counterfactuals!
[^mu_x]: I'm putting unknown values (e.g. $\mu_X$) instead of fixed values (e.g.
1) because the DAG, by itself, does not imply any specific form of the
relationship between the variables. The variables don't have to be normally
distributed, either; I'm assuming a normal distribution for simplicity.
[^intervention_notation]: The notation in this text is a bit different than the
$do(\cdot)$ notation I'm using here. From my reading, Pearl started to use the
$do(\cdot)$ operator in his later work. In the text referenced here,
$p(Y | \hat{X})$ is equal to $p(Y | do(X = x))$.
[^natural_effect]: I won't go into the nuances of why the word "natural" is
doing a fair amount of work here. See 
@pearlIntroductionCausalInference2010,
@pearlInterpretationIdentificationCausal2014 or
@pearlMediationFormulaGuide for more information.
[^natural_direct_eff]: @pearlCausalInferenceStatistics2016 actually define the
natural direct effect as the expected increase in $Y$ when $X$ changes from some
starting value $x_0$ to some value $x$. In our case, we can imagine that $x =
0$, as we're interested in the actual expected value of $Y$, and not the change.
